{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint12課題 深層学習スクラッチ畳み込みニューラルネットワーク1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成\n",
    "チャンネル数を1に限定した1次元畳み込み層のクラスSimpleConv1dを作成してください。基本構造はsprint11で作成したFCクラスと同じになります。なお、重みの初期化に関するクラスは必要に応じて作り変えてください。Xavierの初期値などを使う点は全結合層と同様です。\n",
    "\n",
    "ここではパディングは考えず、ストライドも1に固定します。また、複数のデータを同時に処理することも考えなくて良く、バッチサイズは1のみに対応してください。この部分の拡張はアドバンス課題とします。\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#データセットの用意\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "(784, 1)\n"
     ]
    }
   ],
   "source": [
    "#データセットの用意\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "#reshape\n",
    "test = X_train[0]\n",
    "print(test.shape)\n",
    "test = test.reshape(-1, 1)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#reshape\n",
    "X_train = X_train.reshape(-1, X_train.shape[1]*X_train.shape[2])\n",
    "X_test = X_test.reshape(-1, X_test.shape[1]*X_test.shape[2])\n",
    "#前処理\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.max())\n",
    "print(X_train.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class XavierInitializer():\n",
    "        def __init__(self):\n",
    "                self.sigma = None\n",
    "        def coef(self, n_nodes1,n_nodes2):\n",
    "                self.sigma = 1 / np.power(n_nodes1, 1/2)\n",
    "                W = np.random.normal(loc=0.0, scale=self.sigma, size=(n_nodes1,n_nodes2))\n",
    "                B = np.random.normal(loc=0.0, scale=self.sigma,  size=(1, n_nodes2))\n",
    "                return W, B               \n",
    "        \n",
    "class SimpleConv1d():\n",
    "        \"\"\"\n",
    "        ノード数n_nodes1からn_nodes2への全結合層\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        initializer : 初期化方法のインスタンス\n",
    "        optimizer : 最適化手法のインスタンス\n",
    "        \"\"\"\n",
    "        def __init__(self, n_nodes1,n_nodes2, initializer, optimizer):\n",
    "                self.W, self.B = initializer.coef(n_nodes1, n_nodes2)\n",
    "                self.optimizer = optimizer\n",
    "                \n",
    "                self.dB = None\n",
    "                self.dW = None\n",
    "                \n",
    "        def forward(self, X):\n",
    "                self.pre_Z = np.copy(X)\n",
    "                A = X @ self.W + self.B\n",
    "                return A\n",
    "        \n",
    "        def backward(self, dA):\n",
    "                self.dB = dA\n",
    "                d_pre_Z = dA @ self.W.T\n",
    "                self.dW = np.mean(self.pre_Z.T, axis=1).reshape(-1, 1) @ dA\n",
    "                \n",
    "                self = self.optimizer.update(self)\n",
    "                return d_pre_Z\n",
    "            \n",
    "class StochasticGradientDescent():\n",
    "        def __init__(self, lr):\n",
    "                self.lr = lr\n",
    "        \n",
    "        def update(self, layer):\n",
    "                layer.W = layer.W - self.lr * layer.dW\n",
    "                layer.B = layer.B - self.lr * layer.dB\n",
    "                return layer\n",
    "                \n",
    "class TanH():\n",
    "        def __init__(self):\n",
    "                self.A = None\n",
    "        def forward(self, A):\n",
    "                self.A = np.copy(A)\n",
    "                return  1 - 2 / (1+ np.exp(2 * A))\n",
    "        def backward(self, dZ):\n",
    "                return dZ * (1- np.power(self.forward(np.mean(self.A, axis=0).reshape(1, -1)), 2))\n",
    "\n",
    "class Softmax():\n",
    "        def __init__(self):\n",
    "                pass\n",
    "        def forward(self, A):\n",
    "                e_A = np.exp(A - np.max(A, axis=1, keepdims=True))\n",
    "                return e_A/np.sum(e_A, axis=1, keepdims=True)\n",
    "        \n",
    "        def backward(self, Z, Y):\n",
    "                return np.mean(Z - Y, axis=0).reshape(1, -1)\n",
    "        \n",
    "        def cross_entropy(self, Z, Y):\n",
    "                return -Y * np.log(Z).sum()/Y.shape[0]\n",
    "\n",
    "class Scratch1dCNNClassifier_test():\n",
    "        def __init__(self, lr, batch_size, n_epochs):\n",
    "                self.lr = lr\n",
    "                self.batch_size = batch_size\n",
    "                self.Cnv1 = None\n",
    "                self.activation1 = None\n",
    "                self.n_epochs = n_epochs\n",
    "        \n",
    "        def fit(self, X, y, X_val=None, y_val=None):\n",
    "                optimizer = StochasticGradientDescent(self.lr)\n",
    "                self.Cnv1 = SimpleConv1d(n_nodes1=784, n_nodes2=400, initializer=XavierInitializer(), optimizer=optimizer)\n",
    "                self.activation1 = TanH()\n",
    "                self.Cnv2 = SimpleConv1d(n_nodes1=400, n_nodes2=200, initializer=XavierInitializer(), optimizer=optimizer)\n",
    "                self.activation2 = TanH()\n",
    "                self.Cnv3 = SimpleConv1d(n_nodes1=200, n_nodes2=10, initializer=XavierInitializer(), optimizer=optimizer)\n",
    "                self.activation3 = Softmax()\n",
    "                \n",
    "                mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
    "                eye = np.eye(len(np.unique(y)))\n",
    "                \n",
    "                for epoch in range(self.n_epochs):\n",
    "                        for mini_X, mini_y in mini_batch:\n",
    "                                #forward\n",
    "                                self._forward(mini_X)\n",
    "                                self._backward(eye[mini_y.reshape(-1,)])\n",
    "                \n",
    "        \n",
    "        def predict(self, X):\n",
    "                X = np.array(X)\n",
    "                self._forward(X)\n",
    "                return np.argmax(self.Z3, axis=1)\n",
    "                \n",
    "        def _forward(self, X):\n",
    "                A1 = self.Cnv1.forward(X)\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                A2 = self.Cnv2.forward(Z1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "                A3 = self.Cnv3.forward(Z2)\n",
    "                self.Z3 = self.activation3.forward(A3)\n",
    "                \n",
    "        def _backward(self, y):\n",
    "                dA3 = self.activation3.backward(self.Z3, y)\n",
    "                dZ2 = self.Cnv3.backward(dA3)\n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.Cnv2.backward(dA2)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dZ0 = self.Cnv1.backward(dA1)  \n",
    "\n",
    "class GetMiniBatch:\n",
    "        \"\"\"\n",
    "        ミニバッチを取得するイテレータ\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          学習データ\n",
    "        y : 次の形のndarray, shape (n_samples, 1)\n",
    "          正解値\n",
    "        batch_size : int\n",
    "          バッチサイズ\n",
    "        seed : int\n",
    "          NumPyの乱数のシード\n",
    "        \"\"\"\n",
    "        def __init__(self, X, y, batch_size=10, seed=0):\n",
    "                self.batch_size = batch_size\n",
    "                np.random.seed(seed)\n",
    "                shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "                self.X = X[shuffle_index]\n",
    "                self.y = y[shuffle_index]\n",
    "                self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "\n",
    "        def __len__(self):\n",
    "                return self._stop\n",
    "\n",
    "        def __getitem__(self,item):\n",
    "                p0 = item*self.batch_size\n",
    "                p1 = item*self.batch_size + self.batch_size\n",
    "                return self.X[p0:p1], self.y[p0:p1]        \n",
    "\n",
    "        def __iter__(self):\n",
    "                self._counter = 0\n",
    "                return self\n",
    "\n",
    "        def __next__(self):\n",
    "                if self._counter >= self._stop:\n",
    "                        raise StopIteration()\n",
    "                p0 = self._counter*self.batch_size\n",
    "                p1 = self._counter*self.batch_size + self.batch_size\n",
    "                self._counter += 1\n",
    "                return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 ... 4 5 6]\n",
      "[7 2 1 ... 4 5 6]\n",
      "[ True  True  True ...  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "clf = Scratch1dCNNClassifier_test(lr=0.01, batch_size=20, n_epochs=10)\n",
    "clf.fit(X_train, y_train)\n",
    "pred = clf.predict(X_test)\n",
    "print(y_test)\n",
    "print(pred)\n",
    "print(y_test == pred)\n",
    "print((y_test == pred).sum()/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class XavierInitializer():\n",
    "        def __init__(self):\n",
    "                self.sigma = None\n",
    "        def coef(self, n_nodes1,n_nodes2):\n",
    "                self.sigma = 1 / np.power(n_nodes1, 1/2)\n",
    "                W = np.random.normal(loc=0.0, scale=self.sigma, size=(n_nodes1,n_nodes2))\n",
    "                B = np.random.normal(loc=0.0, scale=self.sigma,  size=(1, n_nodes2))\n",
    "                return W, B               \n",
    "        \n",
    "class SimpleConv1d():\n",
    "        \"\"\"\n",
    "        ノード数n_nodes1からn_nodes2への全結合層\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        initializer : 初期化方法のインスタンス\n",
    "        optimizer : 最適化手法のインスタンス\n",
    "        \"\"\"\n",
    "        def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "                self.W, self.B = initializer.coef(n_nodes1=n_nodes1, n_nodes2=n_nodes2)\n",
    "                self.optimizer = optimizer\n",
    "                \n",
    "                self.dB = None\n",
    "                self.dW = None\n",
    "                \n",
    "        def forward(self, X):\n",
    "                self.pre_Z = np.copy(X)\n",
    "                A = X @ self.W + self.B\n",
    "                return A\n",
    "        \n",
    "        def backward(self, dA):\n",
    "                self.dB = dA\n",
    "                d_pre_Z = dA @ self.W.T\n",
    "                print(\"d_pre_Z\", d_pre_Z.shape)\n",
    "                print(\"dA\", dA.shape)\n",
    "                print(\"self.W\", self.W.shape)\n",
    "                print(\"self.pre_Z\", self.pre_Z.shape)\n",
    "                print(\"pre_Z\", self.pre_Z.reshape(self.pre_Z.shape[0] * self.pre_Z.shape[1], -1))\n",
    "                print(\"dA\", dA.reshape(dA.shape[0] * dA.shape[1], -1))\n",
    "                self.dW = (self.pre_Z.reshape(self.pre_Z.shape[0] * self.pre_Z.shape[1], -1)).T @ (dA.reshape(dA.shape[0] * dA.shape[1], -1))\n",
    "                \n",
    "                self = self.optimizer.update(self)\n",
    "                return d_pre_Z\n",
    "            \n",
    "class StochasticGradientDescent():\n",
    "        def __init__(self, lr):\n",
    "                self.lr = lr\n",
    "        \n",
    "        def update(self, layer):\n",
    "                layer.W = layer.W - self.lr * layer.dW\n",
    "                layer.B = layer.B - self.lr * layer.dB\n",
    "                return layer\n",
    "                \n",
    "class TanH():\n",
    "        def __init__(self):\n",
    "                self.A = None\n",
    "        def forward(self, A):\n",
    "                self.A = np.copy(A)\n",
    "                return  1 - 2 / (1+ np.exp(2 * A))\n",
    "        def backward(self, dZ):\n",
    "                return dZ * (1- np.power(self.forward(self.A), 2))\n",
    "\n",
    "class Softmax():\n",
    "        def __init__(self):\n",
    "                pass\n",
    "        def forward(self, A):\n",
    "                e_A = np.exp(A - np.max(A, axis=1, keepdims=True))\n",
    "                return e_A/np.sum(e_A, axis=1, keepdims=True)\n",
    "        \n",
    "        def backward(self, Z, Y):\n",
    "                #np.mean(Z - Y, axis=0).reshape(1, -1)\n",
    "                return Z - Y\n",
    "        \n",
    "        def cross_entropy(self, Z, Y):\n",
    "                return -Y * np.log(Z).sum()/Y.shape[0]\n",
    "\n",
    "class Scratch1dCNNClassifier():\n",
    "        def __init__(self, lr, batch_size, n_epochs, filter_size=[5, 3]):\n",
    "                self.lr = lr\n",
    "                self.batch_size = batch_size\n",
    "                self.Cnv1 = None\n",
    "                self.activation1 = None\n",
    "                self.n_epochs = n_epochs\n",
    "                self.filter_size = filter_size\n",
    "                self.n_features = None\n",
    "                self.stride_size = 1\n",
    "                self.Z1 = None\n",
    "                self.output_size = None\n",
    "\n",
    "        \n",
    "        def fit(self, X, y, X_val=None, y_val=None):\n",
    "                optimizer = StochasticGradientDescent(self.lr)\n",
    "                self.Cnv1 = SimpleConv1d(n_nodes1=self.filter_size[0], n_nodes2=self.filter_size[1], initializer=XavierInitializer(), optimizer=optimizer)\n",
    "                self.activation1 = TanH()\n",
    "                self.Cnv2 = SimpleConv1d(n_nodes1=self.filter_size[1], n_nodes2=1, initializer=XavierInitializer(), optimizer=optimizer)\n",
    "                self.activation2 = Softmax()\n",
    "                \n",
    "                eye = np.eye(len(np.unique(y)))\n",
    "                self.n_features = X.shape[1]\n",
    "                \n",
    "                self._forward(X)\n",
    "                self._backward(y)\n",
    "                \"\"\"\n",
    "                for epoch in range(self.n_epochs):\n",
    "                        for mini_X, mini_y in mini_batch:\n",
    "                                #forward\n",
    "                                self._forward(mini_X)\n",
    "                                self._backward(eye[mini_y.reshape(-1,)])\n",
    "                \"\"\"\n",
    "        \n",
    "        def predict(self, X):\n",
    "                X = np.array(X)\n",
    "                self._forward(X)\n",
    "\n",
    "                return np.argmax(self.Z2, axis=1)\n",
    "        \n",
    "        def _gen_window_indices(self, filter_size):\n",
    "                indices = np.empty((self.output_size, filter_size))#2, 1, 3\n",
    "                for slide in range(self.output_size):\n",
    "                        indices[slide] = np.arange(slide, filter_size + slide)\n",
    "                return indices.astype(np.int64)\n",
    "                \n",
    "        def _get_output_size(self, X, input_size, padding_size, filter_size, stride_size):\n",
    "                    self.output_size = int((input_size - filter_size + 1) / self.stride_size) #scaler\n",
    "                    return\n",
    "        \n",
    "        def _forward(self, X):\n",
    "                #A1 = np.empty((self.output_size, ))\n",
    "                #slideごとにX上にできる窓を作る\n",
    "                #X_windows = self._gen_windows(X)#shape(output_size, batch_size, filter_size)\n",
    "                \n",
    "                #filter size分のindexをoutput_size分作ってshape(output_size, filter_size)に\n",
    "                #X = [a, b, c, d, e, f, g], filter_size= 3なら [0,1,2], [1,2,3], [2,3,4],[3,4,5], [4,5,6], [5,6,7]を作る\n",
    "                \n",
    "                #slide数(output_size)\n",
    "                self._get_output_size(\n",
    "                        X=X,\n",
    "                        input_size=self.n_features,\n",
    "                        padding_size=0,\n",
    "                        filter_size=self.filter_size[0],\n",
    "                        stride_size=1\n",
    "                )\n",
    "                \n",
    "                window_indices_1 = self._gen_window_indices(self.filter_size[0])#shape(output_size, filter_size)\n",
    "                A1 = np.empty((self.output_size, self.batch_size, self.filter_size[1]))\n",
    "                \n",
    "                A2 = np.empty((self.output_size, self.batch_size, 1))   \n",
    "                for slide, window in enumerate(window_indices_1):\n",
    "                        A1[slide] = self.Cnv1.forward(X[:, window])\n",
    "                \n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                self._get_output_size(\n",
    "                        X=X,\n",
    "                        input_size=self.n_features,\n",
    "                        padding_size=0,\n",
    "                        filter_size=self.filter_size[0],\n",
    "                        stride_size=1\n",
    "                )\n",
    "                window_indices_2 = self._gen_window_indices(self.filter_size[1])\n",
    "                for slide, window in enumerate(window_indices_1):\n",
    "                        A1[slide] = self.Cnv1.forward(X[:, window])\n",
    "                A2 = self.Cnv2.forward(Z1)\n",
    "                self.Z2 = self.activation2.forward(A2)\n",
    "        \n",
    "        def _backward(self, y):\n",
    "                dA2 = self.activation2.backward(self.Z2, y)\n",
    "                print(\"dA2\", dA2.shape)\n",
    "                dZ1 = self.Cnv2.backward(dA2)  \n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "\n",
    "class GetMiniBatch:\n",
    "        \"\"\"\n",
    "        ミニバッチを取得するイテレータ\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          学習データ\n",
    "        y : 次の形のndarray, shape (n_samples, 1)\n",
    "          正解値\n",
    "        batch_size : int\n",
    "          バッチサイズ\n",
    "        seed : int\n",
    "          NumPyの乱数のシード\n",
    "        \"\"\"\n",
    "        def __init__(self, X, y, batch_size=10, seed=0):\n",
    "                self.batch_size = batch_size\n",
    "                np.random.seed(seed)\n",
    "                shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "                self.X = X[shuffle_index]\n",
    "                self.y = y[shuffle_index]\n",
    "                self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "\n",
    "        def __len__(self):\n",
    "                return self._stop\n",
    "\n",
    "        def __getitem__(self,item):\n",
    "                p0 = item*self.batch_size\n",
    "                p1 = item*self.batch_size + self.batch_size\n",
    "                return self.X[p0:p1], self.y[p0:p1]        \n",
    "\n",
    "        def __iter__(self):\n",
    "                self._counter = 0\n",
    "                return self\n",
    "\n",
    "        def __next__(self):\n",
    "                if self._counter >= self._stop:\n",
    "                        raise StopIteration()\n",
    "                p0 = self._counter*self.batch_size\n",
    "                p1 = self._counter*self.batch_size + self.batch_size\n",
    "                self._counter += 1\n",
    "                return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA2 (0, 2, 1)\n",
      "d_pre_Z (0, 2, 3)\n",
      "dA (0, 2, 1)\n",
      "self.W (3, 1)\n",
      "self.pre_Z (0, 2, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 0 into shape (0,newaxis)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-492-e7d2c6e262b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScratch1dCNNClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-490-a2aa92973c87>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, X_val, y_val)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                 \"\"\"\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-490-a2aa92973c87>\u001b[0m in \u001b[0;36m_backward\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mdA2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dA2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdA2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                 \u001b[0mdZ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCnv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m                 \u001b[0mdA1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-490-a2aa92973c87>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dA)\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self.W\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self.pre_Z\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_Z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pre_Z\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_Z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_Z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_Z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dA\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_Z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_Z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_Z\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 0 into shape (0,newaxis)"
     ]
    }
   ],
   "source": [
    "cnn = Scratch1dCNNClassifier(lr=0.01, batch_size=2, n_epochs=10, filter_size=[5,3])\n",
    "cnn.fit(X, y)\n",
    "cnn.predict( np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4]]\n",
      "(1, 4)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = np.array([\n",
    "    [1, 2, 3, 4]\n",
    "])\n",
    "\"\"\"\n",
    "X = np.array([\n",
    "    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n",
    "])\n",
    "\"\"\"\n",
    "y = np.array([\n",
    "    [0],\n",
    "    [1]\n",
    "])\n",
    "X_size = X.shape[1]\n",
    "print(X)\n",
    "print(X.shape)\n",
    "print(X_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2],\n",
       "       [ 5,  6],\n",
       "       [ 9, 10],\n",
       "       [13, 14],\n",
       "       [17, 18]])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:, 0:2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3]\n",
      " [5]\n",
      " [7]]\n",
      "(3, 1)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "w = np.array([\n",
    "    [3],\n",
    "    [5],\n",
    "    [7]\n",
    "])\n",
    "f_size = w.shape[0]\n",
    "print(w)\n",
    "print(w.shape)\n",
    "print(f_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 35.],\n",
       "        [ 95.],\n",
       "        [155.],\n",
       "        [215.],\n",
       "        [275.]],\n",
       "\n",
       "       [[ 50.],\n",
       "        [110.],\n",
       "        [170.],\n",
       "        [230.],\n",
       "        [290.]]])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.empty((X_size - f_size + 1, len(X), 1))\n",
    "\n",
    "for s in range(X_size - f_size + 1):\n",
    "        a[s] = X[:, s:(s + f_size)] @ w + b\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題2】1次元畳み込み後の出力サイズの計算\n",
    "畳み込みを行うと特徴量の数が変化します。どのように変化するかは以下の数式から求められます。パディングやストライドも含めています。この計算を行う関数を作成してください\n",
    "$$\n",
    "N_{out} =  \\frac{N_{in}+2P-F}{S} + 1\\\\\n",
    "$$\n",
    "$N_{out}$: 出力のサイズ（特徴量の数）\n",
    "\n",
    "$N_in$ : 入力のサイズ（特徴量の数）\n",
    "\n",
    "$P$ : ある方向へのパディングの数\n",
    "\n",
    "$F$ : フィルタのサイズ\n",
    "\n",
    "$S$ : ストライドのサイズ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題3】小さな配列での1次元畳み込み層の実験\n",
    "次に示す小さな配列でフォワードプロパゲーションとバックプロパゲーションが正しく行えているか確認してください。\n",
    "\n",
    "入力x、重みw、バイアスbを次のようにします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成\n",
    "\n",
    "チャンネル数を1に限定しない1次元畳み込み層のクラスConv1dを作成してください。\n",
    "\n",
    "紙やホワイトボードを使い計算グラフを書きながら考えてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題5】学習・推定\n",
    "これまで使ってきたニューラルネットワークの全結合層の一部をConv1dに置き換えて学習と推定を行ってください。出力層だけは全結合層をそのまま使ってください。\n",
    "\n",
    "チャンネルが複数ある状態では全結合層への入力は行えません。その段階でのチャンネルは1になるようにするか、平滑化を行います。平滑化はNumPyのreshapeが使用できます。\n",
    "\n",
    "numpy.reshape — NumPy v1.15 Manual\n",
    "\n",
    "画像に対しての1次元畳み込みは実用上は行わないことのため、精度は問いません。\n",
    "\n",
    "## 【問題6】（アドバンス課題）パディングの実装\n",
    "畳み込み層にパディングを加えてください。1次元配列の場合、前後にn個特徴量を増やせるようにしてください。\n",
    "\n",
    "最も単純なパディングは全て0で埋めるゼロパディングであり、CNNでは一般的です。他に端の値を繰り返す方法などもあります。\n",
    "\n",
    "フレームワークによっては、元の入力のサイズを保つようにという指定をすることができます。この機能も持たせておくと便利です。\n",
    "\n",
    "なお、NumPyにはパディングの関数が存在します。\n",
    "\n",
    "numpy.pad — NumPy v1.15 Manual\n",
    "\n",
    "## 【問題7】（アドバンス課題）ミニバッチへの対応\n",
    "ここまでの課題はバッチサイズ1で良いとしてきました。しかし、実際は全結合層同様にミニバッチ学習が行われます。Conv1dクラスを複数のデータが同時に計算できるように変更してください。\n",
    "\n",
    "## 【問題8】（アドバンス課題）任意のストライド数\n",
    "ストライドは1限定の実装をしてきましたが、任意のストライド数に対応できるようにしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
